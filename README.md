# Optimizing an ML Pipeline in Azure
## by Cristian Alberch as part of Udacity Nanodegree "Machine Learning Engineer with Azure"

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Useful Resources
- [ScriptRunConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.scriptrunconfig?view=azure-ml-py)
- [Configure and submit training runs](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-set-up-training-targets)
- [HyperDriveConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py)
- [How to tune hyperparamters](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters)


## Summary
This ficticious dataset belongs to a bank and includes 20 different features from its customers (age, job, marital status, loans, etc.). This
dataset is mapped to a binary target (yes/no) to determine the marketing campaign effectiveness.


**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**
pipeline parameter
{'Regularization Strength:': 1.0, 'Max iterations:': 200, 'Accuracy': 0.9146


VotingEnsemble 0.9176                                


## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**

discrete hyperparameters:
Hyperparameter tuning:

```
ps = RandomParameterSampling(
    {
        '--C': choice(0.1,1.0,10),
        '--max_iter': choice(100,200,300)
    }
)
```
**What are the benefits of the parameter sampler you chose?**
wide range - 0.1 to 10


**What are the benefits of the early stopping policy you chose?**

Refernce: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters
"Early stopping of runs will be determined by a BanditPolicy, which stops a run whose primary metric falls outside the slack_factor"

"Bandit policy is based on slack factor/slack amount and evaluation interval. Bandit ends runs when the primary metric isn't within the specified slack factor/slack amount of the most successful run."

"slack_factor or slack_amount: the slack allowed with respect to the best performing training run. "

the primary metric is accuracy. As all the models are seen to be performing with very similar accuracies, a small slack_factor of 0.1 was chosen so that models that significantly deviate can be terminated early.

the evaluation_interval is set to 1 (default) so that it is checked against every model.

delay_evaluation is set to 5, so that the termination policy only starts to apply after the 5th model, as the differences in performance are expected to be more significant during the first model deviations.

```policy = BanditPolicy(slack_factor = 0.1,
            evaluation_interval = 1,
            delay_evaluation = 5)
```



## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**
